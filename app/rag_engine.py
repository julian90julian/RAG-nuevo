from flask import Flask, request, jsonify
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document
import os
from openai import OpenAIError
from dotenv import load_dotenv

# --- Configuraci√≥n Inicial y Carga de Componentes (solo una vez al iniciar la app) ---

# üîê Cargar clave API desde .env
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

if not api_key:
    print("‚ùå Error: No se encontr√≥ la variable OPENAI_API_KEY en el archivo .env")
    # Para entornos de producci√≥n como Railway, es mejor que esto cause un fallo
    # en el inicio si la clave no est√°, ya que la aplicaci√≥n no funcionar√≠a.
    exit(1) # Salir con un c√≥digo de error

# Inicializar Flask
app = Flask(__name__)

# --- Inicializaci√≥n de OpenAI y Langchain ---
chat = None
qa_chain = None
vectorstore = None

try:
    print("üîå Verificando conexi√≥n con OpenAI y configurando Langchain...")
    chat = ChatOpenAI(api_key=api_key, model="gpt-4o-mini", temperature=0.7) # A√±adir temperature es buena pr√°ctica
    
    # üìÑ Leer documento
    ruta_doc = "data/documentos.txt" # Ajusta la ruta si es necesario
    if not os.path.exists(ruta_doc):
        print(f"‚ùå Error: Archivo no encontrado: {ruta_doc}")
        exit(1)

    with open(ruta_doc, encoding="utf-8") as f:
        texto = f.read()

    if not texto.strip():
        print("‚ùå Error: El archivo est√° vac√≠o. Agrega contenido a 'documentos.txt'.")
        exit(1)

    print(f"üìÑ Documento cargado ({len(texto)} caracteres)")

    # üß© Fragmentar texto
    chunks = [Document(page_content=texto[i:i+500]) for i in range(0, len(texto), 500)]
    print(f"‚úÇÔ∏è Fragmentado en {len(chunks)} partes")

    # üîç Embeddings y FAISS
    print("üì¶ Generando embeddings y vectorstore...")
    # Verificar si el √≠ndice ya existe para cargarlo en vez de crearlo
    if os.path.exists("faiss_index"):
        embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
        vectorstore = FAISS.load_local("faiss_index", embedding_model, allow_dangerous_deserialization=True)
        print("‚úÖ Vectorstore FAISS cargado desde disco.")
    else:
        embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
        vectorstore = FAISS.from_documents(chunks, embedding_model)
        vectorstore.save_local("faiss_index")
        print("‚úÖ Vectorstore FAISS creado y guardado en disco.")

    # üîÅ Crear cadena de respuesta
    qa_chain = RetrievalQA.from_chain_type(
        llm=chat,
        chain_type="stuff",
        retriever=vectorstore.as_retriever()
    )
    print("‚úÖ Cadena de QA de Langchain configurada.")

except OpenAIError as e:
    print(f"‚ùå Error al conectar o configurar OpenAI: {e}")
    exit(1)
except Exception as e:
    print(f"‚ùå Error general durante la inicializaci√≥n de Langchain: {e}")
    exit(1)

# --- Rutas de la API Flask ---

# Ruta ra√≠z para verificar que el servidor est√° funcionando
@app.route("/")
def home():
    if qa_chain:
        return "¬°Servidor Flask para Glamping con IA funcionando correctamente!"
    else:
        return "Servidor Flask funcionando, pero la IA no se inicializ√≥ correctamente.", 500

# Ruta para responder preguntas
@app.route("/ask", methods=["POST"])
def ask_question():
    if not qa_chain:
        return jsonify({"error": "La cadena de IA no est√° inicializada."}), 500

    data = request.get_json()
    pregunta = data.get("question")

    if not pregunta:
        return jsonify({"error": "Por favor, proporciona una pregunta en el cuerpo de la solicitud (campo 'question')."}), 400

    try:
        print(f"ü§ñ Recibida pregunta de usuario: {pregunta}")
        respuesta = qa_chain.run(pregunta)
        print("‚úÖ Respuesta generada y enviada.")
        return jsonify({"question": pregunta, "answer": respuesta})
    except Exception as e:
        print(f"‚ùå Error al procesar la pregunta: {e}")
        return jsonify({"error": f"Error al procesar la pregunta: {str(e)}"}), 500

# --- Iniciar el servidor Flask (solo para desarrollo local) ---
# En Railway, Gunicorn iniciar√° la aplicaci√≥n por ti, esta secci√≥n se ignora.
if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8080))
    print(f"üöÄ Iniciando servidor Flask en http://0.0.0.0:{port}")
    app.run(host="0.0.0.0", port=port, debug=True) # debug=True solo para desarrollo